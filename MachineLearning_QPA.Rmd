---
title: 'Machine Learning: Qualitative Exercise Modelling and Prediction'
output:
  html_document:
    highlight: pygments
    keep_md: yes
    number_sections: no
    theme: cerulean
---

###1. Introduction

With the explosion of methods to record data on personal performance through various devices that communicate with smart phones, an increasing number of users are using the data to evaluate their own performances - particularly quantitatively.  With data gathered from 6 participants performing dumbbell curls in a controlled environment, the attempt will be to train a predictive model on existing data to predict the qualitative execution of the exercise.  Data was provided by:  

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har.#ixzz3HHJSDehS

###2. Setup

An initial dataset of 19 622 cases of 160 variables was supplied.  An additional test set, missing the response variable ***classe*** was also made available for the final testing of the predicticve function.

For the purposes of this machine learning project, the training set is first subdivided into a **training** (11 776 cases) and **testing** (7 846 cases) set on the  ***classe*** response variable - a random sample of 60% for the training set, and the remaining 40% for the initial testing set.  The testing set has been further subdivided into a **validation** (3 923 cases) set for fine tuning of the predictive model, and a **finalTest** (3 923 cases) set - the cases in the training set being randomly assigned to the validation and finalTest set on a 50/50 basis.

The validation dataset will be used to validate the models prior to running the final model against the finalTest set.

```{r dataloading, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
initialData <- read.table("Data/pml-training.csv", header = TRUE, sep = ",", dec = ".", na.strings = "NA", 
                          stringsAsFactors = FALSE)
library(caret)
set.seed(197607)

inTrain <- createDataPartition(y = initialData$classe, p = 0.6, list = FALSE)
training <- initialData[inTrain, ]
testing <- initialData[-inTrain, ]
inValid <- createDataPartition(y = testing$classe, p = 0.5, list = FALSE)
validation <- testing[inValid, ]
finalTest <- testing[-inValid, ]
set.seed(197607)
```

###3. Initial Data Exploration

A visual inspection of the *training* set highlighted the following:  
  
* The first 7 columns of the dataset contain data that are not directly applicable for use in training the predictive model, namely:  
      **a)** "x" : a numeric value that denotes the observation case (row) number  
      **b)** "user_name" : names of participant in the study  
      **c)** "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp" : measure of time and date when the measurements were taken as the exercise was performed  
      **d)** "new_window", "num_window" : a window period that appears to denote each repitition of the exercise movement with the new_window variable encoding whether it's a
         new exercise repitition, and the num_window variable encoding which repition was measured at that point. 
* Various columns that contain missing data, either coded as empty strings "" or as NA.

###4. Data Clean-up

The following data clean-up and preparation steps were done on all the sets - training, validation & finalTest in order to prepare them for further analysis and modelling:    
  
* removing the non-applicable columns  
* replacing blank/empty string data with NA  
* removing division by 0 error codes (an artefact of the excel csv processing) to NA  
* ensuring that predictor classes are all numeric  
* encoding the response variable as a factor variable    
  
This reduced the available variables in each of the datasets to 153.


```{r PreProcessing, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

## Removing the first 7 columns
## Replacing blanks and div/0 with NAs
## Converting chr classes to numeric
## Converting classe to Factor Variable

## Training data clean-up
modelTrain <- training[, -(1:7)]
for (i in 1:length(names(modelTrain))) {
    lv <- (modelTrain[,i] == "" | modelTrain[,i] == "#DIV/0!") & !is.na(modelTrain[,i])
    modelTrain[lv,i] <- NA
}

for (i in 1:152) {
    modelTrain[,i] <- as.numeric(modelTrain[,i])
}

modelTrain$classe <- as.factor(modelTrain$classe)

## Validation data clean-up
modelValid <- validation[, -(1:7)] 
for (i in 1:length(names(modelValid))) {
    lv <- (modelValid[,i] == "" | modelValid[,i] == "#DIV/0!") & !is.na(modelValid[,i])
    modelValid[lv,i] <- NA
}

for (i in 1:152) {
    modelValid[,i] <- as.numeric(modelValid[,i])
}

modelValid$classe <- as.factor(modelValid$classe)

## Testing data clean-up
modelFinTest <- finalTest[, -(1:7)]
for (i in 1:length(names(modelFinTest))) {
    lv <- (modelFinTest[,i] == "" | modelFinTest[,i] == "#DIV/0!") & !is.na(modelFinTest[,i])
    modelFinTest[lv,i] <- NA
}

for (i in 1:152) {
    modelFinTest[,i] <- as.numeric(modelFinTest[,i])
}

modelFinTest$classe <- as.factor(modelFinTest$classe)
```


###5. Exploratory Analysis of Tidy Data - Dimensionality reduction.

As a general rule, a predictor with zero or near zero variance across all cases is not a good predictor when trying to discriminate between variables for accurate prediction.  A nearZeroVariance calculation was performed on all the predictor variables in the data set in order to identify any predictors that would not contribute to the robustness of the final prediction model.

```{r Exploratory, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
## Setting up parameters for dimension reduction

## Checking for Zero or Near Zero variance
##  Needs to be used on all data sets.
nzv <- nearZeroVar(modelTrain, saveMetrics = TRUE)
lv3 <- nzv[(nzv$zeroVar == TRUE | nzv$nzv == TRUE), ]

table(nzv$zeroVar == TRUE | nzv$nzv == TRUE)
```

There were 29 predictors where there was either zero variance, near zero variance or both in the training data set. The training, validation and finalTest data sets were subset to exclude these predictors. Note that the same predictors were removed from the validation and finalTest sets based on the result of the training analysis, the NZV function was not performed anew on these.

```{r zerVar, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
## susetting on zero variance
modelTrain <- modelTrain[,((nzv$zeroVar == FALSE) & (nzv$nzv == FALSE))]
modelValid <- modelValid[,((nzv$zeroVar == FALSE) & (nzv$nzv == FALSE))]
modelFinTest <- modelFinTest[,((nzv$zeroVar == FALSE) & (nzv$nzv == FALSE))]
```

The resulting data sets each had 124 variables available for further analysis - 123 predictors and one response variable.

A missing data test was run on the training data frame to determine how many of the predictors had missing values.  

```{r missingData, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
## Missing data test.
lv2 <- sapply(modelTrain, function(x) sum(is.na(x)))
table(lv2)
```

A further call was made that any predictor with more than half of the cases (> 5 000) encoded as having missing data, would be removed from the modelling process.  These tests were run on the training dataset, and the same variables identified were removed from the validation and finalTest set as well.  The resulting data sets were saved as modelTrainNA, modelValidNA and modelFinTestNA respectively.

```{r naSubset, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

lvNA <- lv2 < 5000
table(lvNA)

modelTrainNA <- modelTrain[ ,lvNA]
modelValidNA <- modelValid[ ,lvNA]
modelFinTestNA <- modelFinTest[ ,lvNA]
```

This has left a greatly reduced number of predictor variables with sufficient data to use for modelling.

Some of the more interesting patterns observed in the data when plotting a scatter plot matrix on some of the remaining variables:

```{r Plot, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE, fig.height=4, fig.width=4}
library(caret)
par(mfrow=c(2,2))
featurePlot(x = modelTrainNA[, 1:4], modelTrainNA$classe, plot = "pairs", xlab = "")
featurePlot(x = modelTrainNA[, 21:24], modelTrainNA$classe, plot = "pairs", xlab = "")
featurePlot(x = modelTrainNA[, 25:28], modelTrainNA$classe, plot = "pairs", xlab = "")
featurePlot(x = modelTrainNA[, 49:52], modelTrainNA$classe, plot = "pairs", xlab = "")
```

A pdf of scatterplot matrix exploration of the full original dataset can be found on https://github.com/Chaendryn/PracMahineLearn_Project

###5. Training

With the remaining predictor variables (52) and a single response variable, a Random Forest modelling algorithm was selected for training.  The first model was fit on the full 53 variable data set.  

```{r fullRF, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

library(caret)
library(randomForest)

set.seed(197607)

##  Full dataset Random Forest fit
modelFitRF1 <- randomForest(classe ~., data = modelTrainNA)
modelFitRF1
```

The OOB (out of bag) sample error rate for the model is listed as 0.74%.  We expect the out-of-sample error rate to be similar or slightly higher when we run the model against a validation and test set, but it should not be completely out of the ballpark.

###6. Testing the Model

```{r predictValidation, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
## prediction test on validation data
predValRF1 <- predict(modelFitRF1, newdata = validation)
```

Using the model **modelfitRF1** against the ***validation*** data set to predict the response variables for the validation, the resulting confusion matrix is as follows:

```{r ValidationConfuse, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
confusionMatrix(predValRF1, validation$classe)
```

```{r Plot2, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE, fig.height=4, fig.width=4}
library(ggplot2)
predicted <- as.factor(predValRF1)
actual <- as.factor(validation$classe)

par(mfrow=c(1,2))
qplot(predicted, fill = classe, data=validation)
qplot(actual, fill = classe, data=validation)
```

An overall accuracy of 0.9926 provides us with an out-of-sample error rate of just 0.74% on the ***validation***, which is the same as the predicted OOB sample error rate from the model.   

```{r validTable, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
table(predicted, actual)
```

Using the model **modelfitRF1** against the ***fintalTest*** data set to predict the response variables for the ***finalTest***, the resulting confusion matrix is as follows:

```{r testSet, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

## prediction Test on test data
predfinTestRF1 <- predict(modelFitRF1, newdata = finalTest)
confusionMatrix(predfinTestRF1, finalTest$classe)
```

The same test on the ***finalTest*** data indicated a slightly lower out-of-sample error rate of only 0.6%.

```{r Plot3, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE, fig.height=4, fig.width=4}
library(ggplot2)
predicted <- as.factor(predfinTestRF1)
actual <- as.factor(finalTest$classe)

par(mfrow=c(1,2))
qplot(predicted, fill = classe, data=finalTest)
qplot(actual, fill = classe, data=finalTest)
```

```{r testTable, echo=FALSE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
table(predicted, actual)
```

###7. Cross Validation
A random forest cross validation was run to review expected out-of-sample error rates and to see whether there was a possible reduced number or variables that could be reliably used to predict an acceptable level of accuracy.

```{r CV, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

## Random Forest Crossvalidation
predictors <- (length(modelTrainNA)-1)
responses <- as.numeric(length(modelTrainNA))

rfcvMod1 <- rfcv(trainx = modelTrainNA[,1:52], trainy = modelTrainNA[,53])
rfcvMod1$error.cv
```

The result of the cross validation showed an out-of-sample error rate of 0.8832% for the full 53 variable set (52 predictors and 1 response).  In addition, the 1.1974% error rate on the 26 variable data set (25 predictors and 1 response) indicated that this could be a viable alternative model for prediction.
  
Therefore a Principal Component Analysis function was performed on the training set to determine whether it would identify a similar number of predictors that would account for 95% of the variability. 

```{r preProcObj, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

## Further dimension reduction process
preObjPCA <- preProcess(modelTrainNA[,-53], method = c("pca"))
preObjPCA
```

Prediction variables were generated from the preProcessing object in order to prepare a model and test it against the validation and finalTest data sets.

```{r preProcData, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

trainPC <- predict(preObjPCA, newdata = modelTrainNA[ ,1:predictors])
validPC <- predict(preObjPCA, newdata = modelValidNA[ ,1:predictors])
testPC <- predict(preObjPCA, newdata = modelFinTestNA[ ,1:predictors])
```

###8. Alternative Model Test with PCA pre-processing

```{r preProcModel, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}
## Creating model for PCA dimension reduction
set.seed(197607)
modFitRFPre1 <- randomForest(modelTrainNA$classe ~., data = trainPC)
modFitRFPre1
```

The OOB error rate estimate is significantly higher on the reduced sample set.  A test against the validation and finTest data sets will show whether this is a true reflection of the out of sample error rate.

###9. Testing PCA model fit on Validation and FinalTest data

```{r preProcModelTest, echo=TRUE, cache=TRUE, comment=NA, message=FALSE, error=FALSE}

## prediction Test on validation data
predValRFPre1 <- predict(modFitRFPre1, newdata = validPC)
confusionMatrix(predValRFPre1, validation$classe)

## prediction Test on test data
predFinTestRFPre1 <- predict(modFitRFPre1, newdata = testPC)
confusionMatrix(predFinTestRFPre1, finalTest$classe)
```

From the above confusion matrices, it can be seen that the dimensionality reduction from 53 to 26 variables had a minor impact on the overall accuracy of the model and can therefore be used for predicting the outcome of the dumbbell curl excersize.

###10. Conclusion
From the initial 160 variables associated with the outcome of the exercise, a reduced model of 53 variables can be used to predict the outcome on new data with a ~99% accuracy.  26 variables with Principal Component Analysis brings the accuracy down to ~97%, which is still well within the bounds of being reasonable.  

From the cross validation run, a 13 variable data set has an error rate of only 0.014776 which is only marginally larger than the 26 variable error rate of 0.011974.  Halving the number of required variables to provide similar accuracy could be a viable avenue for additional research, especially for rolling this out to a broader market for everyday use.




